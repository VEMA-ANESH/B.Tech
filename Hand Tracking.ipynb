{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33930da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ANESH VEMA\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hand model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Initialize a video capture object (you can also use an image)\n",
    "cap = cv2.VideoCapture(0)  # 0 for the default camera\n",
    "\n",
    "# Define gesture recognition functions\n",
    "def recognize_gesture(hand_landmarks, landmark_indices, threshold=0.0):\n",
    "    # Check if the specified landmarks have similar y-coordinates\n",
    "    y_coordinates = [hand_landmarks.landmark[i].y for i in landmark_indices]\n",
    "    min_y = min(y_coordinates)\n",
    "    max_y = max(y_coordinates)\n",
    "    \n",
    "    if max_y - min_y < threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def recognize_thumbs_up(hand_landmarks):\n",
    "    landmark_indices = [4, 8, 12, 16, 20]  # Indices of thumb and fingers' tips\n",
    "    if recognize_gesture(hand_landmarks, landmark_indices, threshold=0.05):\n",
    "        return \"Thumbs Up\"\n",
    "    return \"No Gesture\"\n",
    "\n",
    "def recognize_thumbs_down(hand_landmarks):\n",
    "    landmark_indices = [4, 8, 12, 16, 20]  # Indices of thumb and fingers' tips\n",
    "    if recognize_gesture(hand_landmarks, landmark_indices, threshold=0.05):\n",
    "        return \"Thumbs Down\"\n",
    "    return \"No Gesture\"\n",
    "\n",
    "def recognize_peace_sign(hand_landmarks):\n",
    "    landmark_indices = [8, 12, 16, 20]  # Indices of fingers' tips (excluding thumb)\n",
    "    if recognize_gesture(hand_landmarks, landmark_indices, threshold=0.1):\n",
    "        return \"Peace Sign\"\n",
    "    return \"No Gesture\"\n",
    "\n",
    "def recognize_fist(hand_landmarks):\n",
    "    landmark_indices = [0, 4, 8, 12, 16]  # Indices of palm and fingers' bases\n",
    "    if recognize_gesture(hand_landmarks, landmark_indices, threshold=0.05):\n",
    "        return \"Fist\"\n",
    "    return \"No Gesture\"\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform hand pose estimation\n",
    "    hand_results = hands.process(rgb_frame)\n",
    "\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            # Draw landmarks for the hand (changed the color to blue)\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                x, y = int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])\n",
    "                cv2.circle(frame, (x, y), 5, (255, 0, 0), -1)  # Blue color\n",
    "\n",
    "            # Recognize the gestures\n",
    "            gesture_thumbs_up = recognize_thumbs_up(hand_landmarks)\n",
    "            gesture_thumbs_down = recognize_thumbs_down(hand_landmarks)\n",
    "            gesture_peace_sign = recognize_peace_sign(hand_landmarks)\n",
    "            gesture_fist = recognize_fist(hand_landmarks)\n",
    "\n",
    "            # Display the recognized gestures\n",
    "            cv2.putText(frame, gesture_thumbs_up, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(frame, gesture_thumbs_down, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(frame, gesture_peace_sign, (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(frame, gesture_fist, (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow('Gesture Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f25b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe808f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
